{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH3302 - Méthodes probabilistes et statistiques pour I.A.\n",
    "#### Polytechnique Montréal\n",
    "\n",
    "\n",
    "### Projet A2024\n",
    "\n",
    "-----\n",
    "\n",
    "# Prédiction de la consommation en carburant de voitures récentes.\n",
    "\n",
    "### Contexte\n",
    "\n",
    "## TODO\n",
    "\n",
    "### Objectif\n",
    "\n",
    "## TODO\n",
    "\n",
    "### Données\n",
    "Les données utilisées pour inférer la consommation de carburant sont les suivantes :\n",
    "\n",
    "## TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"annee\";\"type\";\"nombre_cylindres\";\"cylindree\";\"transmission\";\"boite\";\n",
    "\n",
    "#### pistes:\n",
    "\n",
    "preprocessing:\n",
    "\n",
    "quoi faire avec les variables fortement corrélees, cylindree et nombre cylindres (Supprimer une des deux, soit celle qui a le moins d'impact sur la consommation en carburant, ou les combiner en une seule variable) (ÉTAPE 1)\n",
    "(si par exemple, on voit une augmentation disproportionnée de la consommation en carburant avec la cylindrée, on pourrait penser à les combiner en une seule variable)\n",
    "Si nombre_cylindres est une quantité discrète et cylindree est une mesure continue (en litres), leur produit peut être vu comme une \"capacité moteur totale\", une métrique significative pour des modèles prédictifs.\n",
    "\n",
    "nouvelle variable comme age du vehicule (2024 - year) (ca reduit l'importance de l'année dans les données) (Comparer avec juste l'enlever pour voir si ca ameliore le modele) (ÉTAPE 2)\n",
    "\n",
    "reperer les outliers et les traiter\n",
    "\n",
    "equilibrage des classes (sur ou sous representation des types de vehicules)\n",
    "\n",
    "zscore normalization sur cylindree (ou nombre cylindres selon chat gpt mais pas certain)\n",
    "\n",
    "ordinal encoding\n",
    "one hot encoding (si peu de catégories) : Créez une colonne pour chaque catégorie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Statistics, Dates, Gadfly, Combinatorics, Plots, StatsBase, StatsPlots, Random, StatsModels, GLM, LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = CSV.read(\"../data/raw/train.csv\", DataFrame; delim=\";\")\n",
    "test =  CSV.read(\"../data/raw/test.csv\", DataFrame; delim=\";\") #ne contient pas la varialbe consommation\n",
    "\n",
    "Random.seed!(1234) #pour la reproductibilit\n",
    "\n",
    "ntrain = round(Int, .8*nrow(full_train)) #80% des données pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #échantillonnage aléatoire pour l'entrainement\n",
    "valid_id = setdiff(1:nrow(full_train), train_id) #échantillon de validation. prend celles qui ne sont pas dans l'échantillon d'entrainement\n",
    "\n",
    "train = full_train[train_id, :]  \n",
    "valid = full_train[valid_id, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Étude des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = describe(train)\n",
    "testing_stats = describe(test)\n",
    "print(\"Training Set: \\n\", training_stats)\n",
    "print(\"\\n Testing Set: \\n\", testing_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function safe_parse_float(x)\n",
    "    try\n",
    "        return parse(Float64, x)\n",
    "    catch\n",
    "        return missing\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function one_hot_encode(df, cols, levels_dict)\n",
    "    for col in cols\n",
    "        levels_col = levels_dict[col]\n",
    "        for level in levels_col\n",
    "            new_col = Symbol(string(col) * \"_\" * string(level))\n",
    "            df[!, new_col] = ifelse.(df[!, col] .== level, 1.0, 0.0)\n",
    "        end\n",
    "        # Remove the original column\n",
    "        select!(df, Not(col))\n",
    "    end\n",
    "    return df\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(train)\n",
    "data = dropmissing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé des données\n",
    "println(describe(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrélation entre les variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [:annee, :nombre_cylindres, :cylindree, :consommation]\n",
    "\n",
    "M = cor(Matrix(data[:, numeric_cols]))\n",
    "\n",
    "# Afficher la matrice de corrélation\n",
    "println(\"Matrice de corrélation :\")\n",
    "println(M)\n",
    "\n",
    "# PLOT\n",
    "(n,m) = size(M)\n",
    "heatmap(M, fc=cgrad([:white,:dodgerblue4]), xticks=(1:m,numeric_cols), xrot=90, yticks=(1:m,numeric_cols), yflip=true)\n",
    "annotate!([(j, i, text(round(M[i,j],digits=3), 8,\"Computer Modern\",:black)) for i in 1:n for j in 1:m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `nombre_cylindres` et `cylindree` est très élevée, ce qui indique une forte relation positive. Cela suggère que le nombre de cylindres est fortement associé à la cylindrée des véhicules.\n",
    "\n",
    "2. La corrélation entre `cylindree` et `consommation` est également élevée, montrant qu'une augmentation de la cylindrée est associée à une augmentation de la consommation (par exemple, les moteurs plus gros consomment plus de carburant).\n",
    "\n",
    "3. Une corrélation similaire existe entre `nombre_cylindres` et `consommation`, ce qui est logique, car le nombre de cylindres et la cylindrée sont liés.\n",
    "\n",
    "4. Les corrélations entre annee et les autres variables sont faibles et négatives, indiquant que les variables comme le nombre de cylindres, la cylindrée et la consommation ont légèrement diminué avec le temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consommation par type de véhicule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "Gadfly.plot(train, x=:type, y=:consommation, Geom.boxplot )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = unique(skipmissing(data[:, :type]))\n",
    "occurences = [sum(skipmissing(data[:, :type]) .== category) for category in unique_categories]\n",
    "occurences = DataFrame(category = unique_categories, occurences = occurences)\n",
    "occurences = occurences[occurences.occurences .> 10, :] #TODO INVESTIGATE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule moyen :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "vehicule_moyenne = filter(row -> row.type == \"voiture_moyenne\", data)\n",
    "Gadfly.plot(vehicule_moyenne, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type VUS_petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "vehicule_VUSp = filter(row -> row.type == \"VUS_petit\", data)\n",
    "Gadfly.plot(vehicule_VUSp, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_compacte = filter(row -> row.type == \"voiture_compacte\", data)\n",
    "Gadfly.plot(voiture_compacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule 2 places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_deux_places = filter(row -> row.type == \"voiture_deux_places\", data)\n",
    "Gadfly.plot(voiture_deux_places, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule camionnette standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "camionnette_standard = filter(row -> row.type == \"camionnette_standard\", data)\n",
    "Gadfly.plot(camionnette_standard, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule mini compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_minicompacte = filter(row -> row.type == \"voiture_minicompacte\", data)\n",
    "Gadfly.plot(voiture_minicompacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule VUS standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "VUS_standard = filter(row -> row.type == \"VUS_standard\", data)\n",
    "Gadfly.plot(VUS_standard, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule sous-compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_sous_compacte = filter(row -> row.type == \"voiture_sous_compacte\", data)\n",
    "Gadfly.plot(voiture_sous_compacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consommation par cylindrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consommation par nombre de cylindres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consommation par année //TODO METTRE UNE NOTE COMME QUOI PAS BESOIN D'INVESTIGUER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Régression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random.seed!(1234) #pour la reproductibilité\n",
    "\n",
    "# ntrain = round(Int, .8*nrow(full_train)) #80% des données pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "# train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #échantillonnage aléatoire pour l'entrainement\n",
    "# valid_id = setdiff(1:nrow(full_train), train_id) #échantillon de validation. prend celles qui ne sont pas dans l'échantillon d'entrainement\n",
    "\n",
    "# train = full_train[train_id, :]  \n",
    "# valid = full_train[valid_id, :]\n",
    "\n",
    "# first(train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacite_moteur = :nombre_cylindres * :cylindree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert annee column into age\n",
    "train.age = 2024 .- train.annee\n",
    "valid.age = 2024 .- valid.annee\n",
    "test.age = 2024 .- test.annee\n",
    "\n",
    "train = select!(train, Not(:annee))\n",
    "valid = select!(valid, Not(:annee))\n",
    "test = select!(test, Not(:annee))\n",
    "\n",
    "## drop missing values\n",
    "train = dropmissing(train)\n",
    "valid = dropmissing(valid)\n",
    "test = dropmissing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [train, valid]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [train, valid, test]\n",
    "    dropmissing!(df)\n",
    "end\n",
    "\n",
    "# # Encode 'boite' column in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.boite = ifelse.(df.boite .== \"automatique\", 1.0, 0.0)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "function normalize(df, cols)\n",
    "    for col in cols\n",
    "        df[!, col] = (df[!, col] .- mean(df[!, col])) ./ std(df[!, col])\n",
    "    end\n",
    "    return df\n",
    "end\n",
    "\n",
    "# cols_to_normalize = [:cylindree, :age, :nombre_cylindres]\n",
    "# train = normalize(train, cols_to_normalize)\n",
    "# valid = normalize(valid, cols_to_normalize)\n",
    "# test = normalize(test, cols_to_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GLM.lm(@formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree), train)\n",
    "# Prediction avec l'ensemble de validation\n",
    "valid_prediction = GLM.predict(model, valid)\n",
    "# Trouver la moyenne de prediction\n",
    "mean_prediction = mean(valid_prediction)\n",
    "# Remplacer les missing par la moyenne\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "# Transformer les predictions en valeur entiere\n",
    "#v = Int.(round.(valid_prediction, digits=0)) #mettre une commentaire sur la difference que ca entraine sur le rmse\n",
    "# Calculer le RMSE\n",
    "rmse_valid = sqrt(mean((valid_prediction - valid.consommation).^2))\n",
    "println(\"RMSE: \", rmse_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nrow(test)\n",
    "\n",
    "id = 1:n\n",
    "\n",
    "ŷ = GLM.predict(model, test)\n",
    "\n",
    "df_pred = DataFrame(id=id, consommation=ŷ)\n",
    "\n",
    "name = \"linear/\" * string(rmse_valid) * \".csv\"\n",
    "CSV.write(\"../submissions/\" * name, df_pred)\n",
    "println(\"Predictions exported successfully to \" * name*\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Régression bayesienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert annee column into age\n",
    "train.age = 2024 .- train.annee\n",
    "valid.age = 2024 .- valid.annee\n",
    "test.age = 2024 .- test.annee\n",
    "\n",
    "train = select!(train, Not(:annee))\n",
    "valid = select!(valid, Not(:annee))\n",
    "test = select!(test, Not(:annee))\n",
    "\n",
    "## drop missing values\n",
    "train = dropmissing(train)\n",
    "valid = dropmissing(valid)\n",
    "test = dropmissing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [train, valid]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [train, valid, test]\n",
    "    dropmissing!(df)\n",
    "end\n",
    "\n",
    "# Encode 'boite' column in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.boite = ifelse.(df.boite .== \"automatique\", 1.0, 0.0)\n",
    "end\n",
    "\n",
    "# #cols_to_normalize = [:cylindree, :age, :nombre_cylindres]\n",
    "# train = normalize(train, cols_to_normalize)\n",
    "# valid = normalize(valid, cols_to_normalize)\n",
    "# test = normalize(test, cols_to_normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.consommation\n",
    "X_train = select(train, Not(:consommation))\n",
    "y_valid = valid.consommation\n",
    "X_valid = select(valid, Not(:consommation))\n",
    "X_test = deepcopy(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric feature indices\n",
    "feature_names = names(train)\n",
    "numeric_features = [ :cylindree, :nombre_cylindres, :age]\n",
    "numeric_indices = findall(x -> x in numeric_features, feature_names)\n",
    "\n",
    "means = mean(Matrix(X_train[:, numeric_features]), dims=1)\n",
    "stds = std(Matrix(X_train[:, numeric_features]), dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function standardizer(X, means, stds)\n",
    "    X = deepcopy(X)\n",
    "    for j in 1:size(X, 2)\n",
    "        if j in numeric_indices\n",
    "            X[:, j] = (X[:, j] .- means[j]) ./ stds[j]\n",
    "        end\n",
    "    end\n",
    "    return X\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = standardizer(Matrix(X_train), means, stds)\n",
    "X_valid = standardizer(Matrix(X_valid), means, stds)\n",
    "X_test = standardizer(Matrix(X_test), means, stds)\n",
    "\n",
    "y_train = Vector(y_train)\n",
    "y_valid = Vector(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression with cross-validation\n",
    "XtX = X_train' * X_train\n",
    "Xty = X_train' * y_train\n",
    "n_features = size(X_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values = 10 .^ range(-5, stop=5, length=1000)\n",
    "best_rmse = Inf\n",
    "best_lambda = 0.0\n",
    "best_beta = nothing\n",
    "\n",
    "\n",
    "for λ in lambda_values\n",
    "    beta = (XtX + λ * I) \\ Xty\n",
    "    y_pred_valid = X_valid * beta\n",
    "    rmse = sqrt(mean((y_pred_valid - y_valid).^2))\n",
    "    println(\"Lambda: \", λ, \" RMSE: \", rmse)\n",
    "    if rmse < best_rmse\n",
    "        best_rmse = rmse\n",
    "        best_lambda = λ\n",
    "        best_beta = beta\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Best Lambda: \", best_lambda)\n",
    "println(\"Best RMSE: \", best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation on validation set\n",
    "y_valid_pred = X_valid * best_beta\n",
    "rmse_valid = sqrt(mean((y_valid_pred - y_valid).^2))\n",
    "println(\"Validation RMSE: \", rmse_valid)\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred = X_test * best_beta\n",
    "\n",
    "# Prepare submission DataFrame\n",
    "n_test = size(y_test_pred, 1)\n",
    "id = 1:n_test\n",
    "df_pred = DataFrame(id=id, consommation=y_test_pred)\n",
    "\n",
    "name = \"ridge\" * string(rmse_valid) * \".csv\"\n",
    "CSV.write(\"../submissions/\" * name, df_pred)\n",
    "println(\"Predictions exported successfully to \" * name*\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation par k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k_folds = vcat(train, valid)\n",
    "y = data_k_folds.consommation\n",
    "X = select(data_k_folds, Not(:consommation))\n",
    "\n",
    "n = nrow(data_k_folds)\n",
    "k = 5  \n",
    "fold_size = n ÷ k\n",
    "\n",
    "indices = randperm(n)\n",
    "\n",
    "rms_scores = []\n",
    "\n",
    "for i in 0:(k-1)\n",
    "    test_indices = indices[(i*fold_size + 1):min((i+1)*fold_size, n)]\n",
    "    train_indices = setdiff(indices, test_indices)\n",
    "    \n",
    "    train_data = data_k_folds[train_indices, :]\n",
    "    test_data = data_k_folds[test_indices, :]\n",
    "    \n",
    "    model = lm(@formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree), data_k_folds)\n",
    " \n",
    "    \n",
    "    valid_prediction = GLM.predict(model, test_data)\n",
    "    \n",
    "    mean_prediction = mean(skipmissing(valid_prediction))\n",
    "    valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "    \n",
    "    if any(ismissing, valid_prediction)\n",
    "        error(\"Skip les valeur missing\")\n",
    "    end\n",
    "    \n",
    "    v = max.(valid_prediction, 0) \n",
    "    \n",
    "    score = sqrt(mean((v - test_data.consommation).^2))\n",
    "    push!(rms_scores, score)\n",
    "end\n",
    "\n",
    "moyenne_rmse = mean(rms_scores)\n",
    "println(\"Moyenne RMSE : $moyenne_rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression par l'approche des composantes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234) # For reproducibility\n",
    "\n",
    "# Split the data\n",
    "ntrain = round(Int, 0.8 * nrow(full_train))\n",
    "train_id = sample(1:nrow(full_train), ntrain; replace=false, ordered=true)\n",
    "valid_id = setdiff(1:nrow(full_train), train_id)\n",
    "\n",
    "train = full_train[train_id, :]\n",
    "valid = full_train[valid_id, :]\n",
    "\n",
    "# Data cleaning\n",
    "for col in [:cylindree, :consommation]\n",
    "    train[!, col] = replace.(train[!, col], \",\" => \".\")\n",
    "    valid[!, col] = replace.(valid[!, col], \",\" => \".\")\n",
    "    train[!, col] = safe_parse_float.(train[!, col])\n",
    "    valid[!, col] = safe_parse_float.(valid[!, col])\n",
    "end\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train = select(train, Not([:type, :transmission, :boite]))\n",
    "valid = select(valid, Not([:type, :transmission, :boite]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TODO CONCLUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = Vector(train.consommation)\n",
    "X_train = Matrix(train[:, Not([:consommation])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrer - réduire les données\n",
    "X_mean = mean(X_train; dims=1)\n",
    "X_stddev = std(X_train; dims=1, corrected=false)\n",
    "X_train_std = (X_train .- X_mean) ./ X_stddev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décomposer en composantes principales\n",
    "pca_model = fit(PCA, X_train_std'; maxoutdim=8)\n",
    "\n",
    "# T = Z * V pour la matrice des composantes principales.\n",
    "Z_train = MultivariateStats.transform(pca_model, X_train_std')\n",
    "Z_train = Z_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model de regression sur les composantes principales\n",
    "model = lm(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrer - réduire les données de validation\n",
    "X_valid = Matrix(valid[:, Not([:consommation])])\n",
    "\n",
    "X_valid_std = (X_valid .- X_mean) ./ X_stddev\n",
    "\n",
    "Z_valid = MultivariateStats.transform(pca_model, X_valid_std')\n",
    "Z_valid = Z_valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prediction = predict(model, Z_valid)\n",
    "\n",
    "mean_prediction = mean(skipmissing(valid_prediction))\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "\n",
    "mean_actual = mean(skipmissing(valid.consommation))\n",
    "actual_values = coalesce.(valid.consommation, mean_actual)\n",
    "\n",
    "rmse = sqrt(mean((valid_prediction - actual_values).^2))\n",
    "println(\"RMSE: \", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approche de pupuce \n",
    "# données ne sont pas standartisées\n",
    "y = train.consommation\n",
    "X = train[:, Not([:consommation])]\n",
    "\n",
    "X = Matrix(X)\n",
    "y = Vector(y)\n",
    "\n",
    "pca_model = fit(PCA, X, maxoutdim=8)\n",
    "\n",
    "# La PCA est appliquée aux données non standardisées\n",
    "# Cela revient à entraîner le modèle sur une version approximative \n",
    "#des données d'origine, ce qui peut réintroduire la multicolinéarité et annuler certains avantages de la PCA.\n",
    "Yte = predict(pca_model, X)\n",
    "Xr = reconstruct(pca_model, Yte)\n",
    "\n",
    "model = lm(Xr, y) \n",
    "consommation = valid.consommation\n",
    "select!(valid, Not(:consommation));\n",
    "X_valid = Matrix{Float64}(valid);\n",
    "valid_prediction = predict(model, X_valid)\n",
    "\n",
    "mean_prediction = mean(skipmissing(valid_prediction))\n",
    "\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "\n",
    "mean_actual = mean(skipmissing(consommation))\n",
    "actual_values = coalesce.(consommation, mean_actual)\n",
    "\n",
    "rmse = sqrt(mean((valid_prediction - actual_values).^2))\n",
    "println(\"RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x̄ = vec(mean(X, dims=1))\n",
    "\n",
    "Z = X .- x̄'\n",
    "\n",
    "F = svd(Z)\n",
    "V = F.V\n",
    "U = F.U\n",
    "γ = F.S;\n",
    "\n",
    "cumvar = cumsum(γ.^2)\n",
    "\n",
    "ratio = cumvar / cumvar[end]\n",
    "\n",
    "df = DataFrame(k = Int64[], Variance = Float64[])\n",
    "\n",
    "for k in 1:length(ratio)\n",
    "    push!(df, [k, ratio[k]])\n",
    "end\n",
    "\n",
    "Gadfly.plot(df, x=:k, y=:Variance, Geom.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Impact des échelles dans les données\n",
    "Dans les données non standardisées, les variables explicatives ayant des valeurs plus grandes peuvent dominer la variance totale, ce qui oriente la PCA vers ces variables. Si ces variables sont fortement corrélées avec la variable cible (\n",
    "𝑌\n",
    "Y), alors le modèle peut accidentellement mieux capturer cette relation.\n",
    "En revanche, la standardisation neutralise les différences d'échelle, ce qui peut diluer l'effet des variables dominantes, même si elles ont une forte corrélation avec \n",
    "𝑌\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Dans votre jeu de données, il est possible qu'une ou plusieurs variables avec des échelles plus grandes soient prédictives de \n",
    "𝑌\n",
    "Y, et la méthode non standardisée en profite directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Sur-ajustement (Overfitting)\n",
    "La méthode non standardisée applique la PCA sur les données d'origine, mais utilise ensuite les données reconstruites (\n",
    "𝑋\n",
    "𝑟\n",
    "X \n",
    "r\n",
    "​\n",
    " ) pour l'entraînement. Cela peut réintroduire une grande partie des informations originales, y compris le bruit ou les corrélations spurielles, ce qui peut conduire à un modèle sur-ajusté.\n",
    "Si l'ensemble de validation est similaire à l'ensemble d'entraînement (par exemple, s'il provient de la même distribution ou a des caractéristiques similaires), un sur-ajustement peut donner des RMSE artificiellement bas.\n",
    "Explication potentielle :\n",
    "Votre validation pourrait être moins rigoureuse, et la méthode non standardisée exploite des relations non généralisables dans les données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Le fait que la méthode non standardisée donne un meilleur RMSE peut s'expliquer par plusieurs raisons, mais cela ne signifie pas nécessairement qu'elle est meilleure ou qu'elle respecte les principes statistiques sous-jacents. Explorons pourquoi cela pourrait se produire :\n",
    "\n",
    "1. Impact des échelles dans les données\n",
    "Dans les données non standardisées, les variables explicatives ayant des valeurs plus grandes peuvent dominer la variance totale, ce qui oriente la PCA vers ces variables. Si ces variables sont fortement corrélées avec la variable cible (\n",
    "𝑌\n",
    "Y), alors le modèle peut accidentellement mieux capturer cette relation.\n",
    "En revanche, la standardisation neutralise les différences d'échelle, ce qui peut diluer l'effet des variables dominantes, même si elles ont une forte corrélation avec \n",
    "𝑌\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Dans votre jeu de données, il est possible qu'une ou plusieurs variables avec des échelles plus grandes soient prédictives de \n",
    "𝑌\n",
    "Y, et la méthode non standardisée en profite directement.\n",
    "\n",
    "2. Sur-ajustement (Overfitting)\n",
    "La méthode non standardisée applique la PCA sur les données d'origine, mais utilise ensuite les données reconstruites (\n",
    "𝑋\n",
    "𝑟\n",
    "X \n",
    "r\n",
    "​\n",
    " ) pour l'entraînement. Cela peut réintroduire une grande partie des informations originales, y compris le bruit ou les corrélations spurielles, ce qui peut conduire à un modèle sur-ajusté.\n",
    "Si l'ensemble de validation est similaire à l'ensemble d'entraînement (par exemple, s'il provient de la même distribution ou a des caractéristiques similaires), un sur-ajustement peut donner des RMSE artificiellement bas.\n",
    "Explication potentielle :\n",
    "Votre validation pourrait être moins rigoureuse, et la méthode non standardisée exploite des relations non généralisables dans les données.\n",
    "\n",
    "3. Corrélation forte entre les variables\n",
    "Si les variables explicatives ont une forte corrélation intrinsèque, l'analyse en composantes principales standardisée peut répartir cette information sur plusieurs composantes. Cela réduit la capacité du modèle à se concentrer sur des variables fortement corrélées avec \n",
    "𝑌\n",
    "Y.\n",
    "La méthode non standardisée, en revanche, conserve ces corrélations et peut donc mieux modéliser la relation entre \n",
    "𝑋\n",
    "X et \n",
    "𝑌\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Les corrélations fortes dans vos données favorisent la méthode non standardisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Effet des données reconstruites\n",
    "Dans la méthode non standardisée, vous utilisez des données reconstruites (\n",
    "𝑋\n",
    "𝑟\n",
    "X \n",
    "r\n",
    "​\n",
    " ), qui incluent une grande partie de l'information originale. Cela signifie que la régression est moins influencée par la réduction de dimension et plus proche de la régression sur les données d'origine.\n",
    "En revanche, dans la méthode standardisée, seules les composantes principales sont utilisées, ce qui peut sacrifier une partie de l'information pour réduire la multicolinéarité et améliorer la généralisation.\n",
    "Explication potentielle :\n",
    "L'utilisation des données reconstruites dans la méthode non standardisée maintient plus d'information, ce qui peut donner un RMSE plus faible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Problème avec la PCA standardisée\n",
    "Si la standardisation n'est pas appropriée (par exemple, si certaines variables explicatives sont presque constantes ou si elles sont déjà sur des échelles comparables), alors l'analyse en composantes principales standardisée peut ne pas capturer efficacement les directions principales de la variance.\n",
    "Cela peut entraîner une perte d'information utile, ce qui affecte les performances du modèle.\n",
    "Explication potentielle :\n",
    "Les variables de votre jeu de données n'ont peut-être pas besoin d'être standardisées ou la standardisation introduit un biais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
