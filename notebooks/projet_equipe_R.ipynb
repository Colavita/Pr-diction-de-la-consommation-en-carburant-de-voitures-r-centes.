{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH3302 - Méthodes probabilistes et statistiques pour I.A.\n",
    "#### Polytechnique Montréal\n",
    "\n",
    "\n",
    "### Projet A2024\n",
    "\n",
    "-----\n",
    "\n",
    "# Prédiction de la consommation en carburant de voitures récentes.\n",
    "\n",
    "### Contexte\n",
    "\n",
    "#### TODO\n",
    "\n",
    "### Objectif\n",
    "\n",
    "#### TODO\n",
    "\n",
    "### Données\n",
    "Les données utilisées pour inférer la consommation de carburant sont les suivantes :\n",
    "\n",
    "#### TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"annee\";\"type\";\"nombre_cylindres\";\"cylindree\";\"transmission\";\"boite\";\n",
    "\n",
    "#### pistes:\n",
    "\n",
    "preprocessing:\n",
    "\n",
    "quoi faire avec les variables fortement corrélees, cylindree et nombre cylindres (Supprimer une des deux, soit celle qui a le moins d'impact sur la consommation en carburant, ou les combiner en une seule variable) (ÉTAPE 1)\n",
    "(si par exemple, on voit une augmentation disproportionnée de la consommation en carburant avec la cylindrée, on pourrait penser à les combiner en une seule variable)\n",
    "Si nombre_cylindres est une quantité discrète et cylindree est une mesure continue (en litres), leur produit peut être vu comme une \"capacité moteur totale\", une métrique significative pour des modèles prédictifs.\n",
    "\n",
    "nouvelle variable comme age du vehicule (2024 - year) (ca reduit l'importance de l'année dans les données) (Comparer avec juste l'enlever pour voir si ca ameliore le modele) (ÉTAPE 2)\n",
    "\n",
    "reperer les outliers et les traiter\n",
    "\n",
    "equilibrage des classes (sur ou sous representation des types de vehicules)\n",
    "\n",
    "zscore normalization sur cylindree (ou nombre cylindres selon chat gpt mais pas certain)\n",
    "\n",
    "ordinal encoding\n",
    "one hot encoding (si peu de catégories) : Créez une colonne pour chaque catégorie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Partitionnement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " using CSV, DataFrames, Statistics, Dates, Gadfly, Combinatorics, Plots, StatsBase, StatsPlots, Random, StatsModels, GLM, LinearAlgebra, MultivariateStats, Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = CSV.read(\"../data/raw/train.csv\", DataFrame; delim=\";\")\n",
    "test =  CSV.read(\"../data/raw/test.csv\", DataFrame; delim=\";\") #ne contient pas la varialbe consommation\n",
    "\n",
    "Random.seed!(1234) #pour la reproductibilit\n",
    "\n",
    "ntrain = round(Int, .8*nrow(full_train)) #80% des données pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #échantillonnage aléatoire pour l'entrainement\n",
    "valid_id = setdiff(1:nrow(full_train), train_id) #échantillon de validation. prend celles qui ne sont pas dans l'échantillon d'entrainement\n",
    "\n",
    "train = full_train[train_id, :]  \n",
    "valid = full_train[valid_id, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function safe_parse_float(x)\n",
    "    try\n",
    "        return parse(Float64, x)\n",
    "    catch\n",
    "        return missing\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function one_hot_encode(df, cols, levels_dict)\n",
    "    for col in cols\n",
    "        levels_col = levels_dict[col]\n",
    "        for level in levels_col\n",
    "            new_col = Symbol(string(col) * \"_\" * string(level))\n",
    "            df[!, new_col] = ifelse.(df[!, col] .== level, 1.0, 0.0)\n",
    "        end\n",
    "        select!(df, Not(col))\n",
    "    end\n",
    "    return df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default_processing (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function default_prerocessing(data)\n",
    "    # Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [data]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [data]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [data]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [data]\n",
    "    dropmissing!(df)\n",
    "end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par nous familiariser avec les données à notre disposition. Dans cette section, nous allons explorer différentes pistes qui pourraient potentiellement avoir un impact positif sur nos prédictions, que ce soit de répérer des données éloignées du reste de l'ensemble, de répérer des tendances, de détecter des instances de multicolinéarité, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape pour traiter les données est de copier les données d'entraînement dans une variable \"data\" afin d'éviter de corrompre les données originales lors de nos manipulations. On peut ensuite retirer les variables manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>5×7 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">annee</th><th style = \"text-align: left;\">type</th><th style = \"text-align: left;\">nombre_cylindres</th><th style = \"text-align: left;\">cylindree</th><th style = \"text-align: left;\">transmission</th><th style = \"text-align: left;\">boite</th><th style = \"text-align: left;\">consommation</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"String31\" style = \"text-align: left;\">String31</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"String15\" style = \"text-align: left;\">String15</th><th title = \"String15\" style = \"text-align: left;\">String15</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">2023</td><td style = \"text-align: left;\">voiture_moyenne</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">4.4</td><td style = \"text-align: left;\">integrale</td><td style = \"text-align: left;\">automatique</td><td style = \"text-align: right;\">13.8359</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">2020</td><td style = \"text-align: left;\">VUS_petit</td><td style = \"text-align: right;\">4</td><td style = \"text-align: right;\">2.0</td><td style = \"text-align: left;\">integrale</td><td style = \"text-align: left;\">automatique</td><td style = \"text-align: right;\">9.80042</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">2021</td><td style = \"text-align: left;\">voiture_compacte</td><td style = \"text-align: right;\">6</td><td style = \"text-align: right;\">3.3</td><td style = \"text-align: left;\">propulsion</td><td style = \"text-align: left;\">automatique</td><td style = \"text-align: right;\">11.7605</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">2023</td><td style = \"text-align: left;\">voiture_deux_places</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: left;\">integrale</td><td style = \"text-align: left;\">automatique</td><td style = \"text-align: right;\">13.0672</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">2022</td><td style = \"text-align: left;\">voiture_moyenne</td><td style = \"text-align: right;\">8</td><td style = \"text-align: right;\">4.4</td><td style = \"text-align: left;\">integrale</td><td style = \"text-align: left;\">automatique</td><td style = \"text-align: right;\">13.8359</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& annee & type & nombre\\_cylindres & cylindree & transmission & boite & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String31 & Int64 & Float64 & String15 & String15 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 2023 & voiture\\_moyenne & 8 & 4.4 & integrale & automatique & $\\dots$ \\\\\n",
       "\t2 & 2020 & VUS\\_petit & 4 & 2.0 & integrale & automatique & $\\dots$ \\\\\n",
       "\t3 & 2021 & voiture\\_compacte & 6 & 3.3 & propulsion & automatique & $\\dots$ \\\\\n",
       "\t4 & 2023 & voiture\\_deux\\_places & 8 & 5.0 & integrale & automatique & $\\dots$ \\\\\n",
       "\t5 & 2022 & voiture\\_moyenne & 8 & 4.4 & integrale & automatique & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m annee \u001b[0m\u001b[1m type                \u001b[0m\u001b[1m nombre_cylindres \u001b[0m\u001b[1m cylindree \u001b[0m\u001b[1m transmission \u001b[0m\u001b[1m \u001b[0m ⋯\n",
       "     │\u001b[90m Int64 \u001b[0m\u001b[90m String31            \u001b[0m\u001b[90m Int64            \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m String15     \u001b[0m\u001b[90m \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  2023  voiture_moyenne                     8        4.4  integrale      ⋯\n",
       "   2 │  2020  VUS_petit                           4        2.0  integrale\n",
       "   3 │  2021  voiture_compacte                    6        3.3  propulsion\n",
       "   4 │  2023  voiture_deux_places                 8        5.0  integrale\n",
       "   5 │  2022  voiture_moyenne                     8        4.4  integrale      ⋯\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# faire une copie initiale du dataframe\n",
    "data = deepcopy(train)\n",
    "\n",
    "# retirer les colonnes ayant des données vides de notre dataframe\n",
    "data = dropmissing(data)\n",
    "\n",
    "default_prerocessing(data)\n",
    "\n",
    "first(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: \n",
      "\u001b[1m7×7 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m variable         \u001b[0m\u001b[1m mean    \u001b[0m\u001b[1m min         \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max                   \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
      "     │\u001b[90m Symbol           \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any         \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any                   \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
      "─────┼────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   1 │ annee             2019.15  2014         2019.0   2024                          0  Int64\n",
      "   2 │ type             \u001b[90m         \u001b[0m VUS_petit   \u001b[90m         \u001b[0m voiture_sous_compacte         0  String31\n",
      "   3 │ nombre_cylindres  5.25237  3            4.0      12                            0  Int64\n",
      "   4 │ cylindree         2.86467  1.2          2.5      6.4                           0  Float64\n",
      "   5 │ transmission     \u001b[90m         \u001b[0m 4x4         \u001b[90m         \u001b[0m traction                      0  String15\n",
      "   6 │ boite            \u001b[90m         \u001b[0m automatique \u001b[90m         \u001b[0m manuelle                      0  String15\n",
      "   7 │ consommation      10.2898  4.52327      10.2265  16.8007                       0  Float64\n",
      " Testing Set: \n",
      "\u001b[1m6×7 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m variable         \u001b[0m\u001b[1m mean    \u001b[0m\u001b[1m min         \u001b[0m\u001b[1m median \u001b[0m\u001b[1m max                   \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
      "     │\u001b[90m Symbol           \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any         \u001b[0m\u001b[90m Union… \u001b[0m\u001b[90m Any                   \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
      "─────┼───────────────────────────────────────────────────────────────────────────────────────────\n",
      "   1 │ annee             2018.57  2014         2018.0  2024                          0  Int64\n",
      "   2 │ type             \u001b[90m         \u001b[0m VUS_petit   \u001b[90m        \u001b[0m voiture_sous_compacte         0  String31\n",
      "   3 │ nombre_cylindres  5.38667  3            4.0     12                            0  Int64\n",
      "   4 │ cylindree        \u001b[90m         \u001b[0m 1,2         \u001b[90m        \u001b[0m 6,2                           0  String3\n",
      "   5 │ transmission     \u001b[90m         \u001b[0m 4x4         \u001b[90m        \u001b[0m traction                      0  String15\n",
      "   6 │ boite            \u001b[90m         \u001b[0m automatique \u001b[90m        \u001b[0m manuelle                      0  String15"
     ]
    }
   ],
   "source": [
    "#Évaluer s'il y a des différences significatives entre le jeu de données d'entraînement et le jeu de données de test\n",
    "\n",
    "data_stats = describe(data)\n",
    "testing_stats = describe(test)\n",
    "print(\"Training Set: \\n\", data_stats)\n",
    "print(\"\\n Testing Set: \\n\", testing_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables explicatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons commencer par analyser chacune des variables explicatives potentielles en détail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Année"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première variable explicative est l'année de fabrication du véhicule. Nous avons commencé par regarder la tendance de la consommation d'essence au fil des ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(16cm, 12cm)\n",
    "Gadfly.plot(\n",
    "    data,\n",
    "    x=:annee,\n",
    "    y=:consommation,\n",
    "    Geom.boxplot,\n",
    "    Guide.title(\"Consommation d'essence en fonction de l'année du véhicule\"),\n",
    "    Guide.xlabel(\"Année\"),\n",
    "    Guide.ylabel(\"Consommation (L/100km)\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut tirer quelques observations de ce diagramme. D'abord, les données de l'ensemble d'entraînement sont réparties sur une période de seulement 10 ans, soit de 2014 à 2024. Ceci fait en sorte que les consommations d'essence correspondantes varieront peu d'une année à l'autre, car les véhicules ne se sont pas améliorés technologiquement de façon assez significative sur une aussi courte période pour que l'année de fabrication ait une incidence notable sur la consommation d'essence. De plus, lorsqu'on observe le diagramme, il est difficile de remarquer une tendance claire dans la variation de la consommation d'essence sur la période de 10 ans. La consommation d'essence semblait être à la hausse entre 2018 et 2020, puis s'est stabilisée avant de rechuter en 2023. Ces variations nous semblent difficiles à modéliser. Pour toutes ces raisons, il nous semble initialement que l'année de fabrication soit une variable explicative de faible impact sur les prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous nous sommes questionné quant à la grosseur des chiffres reliées à l'année dans un modèle. En effet, les autres données numériques étant le nombre de cylindres et la cylindrée, qui ont tous les deux des valeurs nettement inférieures à un chiffre comme 2020, nous nous demandions si cette disparité pouvait avoir un impact sur la qualité de nos prédictions. Nous avons donc opté pour la stratégie de changer la variable explicative de l'année de fabrication pour une variable d'âge, qui serait entre 1 et 10 ans, et qui serait beaucoup plus proche des valeurs des autres variables explicatives. Ci-dessous se trouve notre façon de procéder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.age = 2024 .- data.annee\n",
    "\n",
    "select!(data, Not(:annee))\n",
    "first(data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé des données\n",
    "println(describe(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type de véhicule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "Gadfly.plot(train, x=:type, y=:consommation, Geom.boxplot )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categories = unique(skipmissing(data[:, :type]))\n",
    "occurences = [sum(skipmissing(data[:, :type]) .== category) for category in unique_categories]\n",
    "occurences = DataFrame(category = unique_categories, occurences = occurences)\n",
    "# occurences = occurences[occurences.occurences .> 10, :] #TODO INVESTIGATE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule moyen :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "vehicule_moyenne = filter(row -> row.type == \"voiture_moyenne\", data)\n",
    "Gadfly.plot(vehicule_moyenne, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type VUS_petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "vehicule_VUSp = filter(row -> row.type == \"VUS_petit\", data)\n",
    "Gadfly.plot(vehicule_VUSp, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_compacte = filter(row -> row.type == \"voiture_compacte\", data)\n",
    "Gadfly.plot(voiture_compacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule 2 places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_deux_places = filter(row -> row.type == \"voiture_deux_places\", data)\n",
    "Gadfly.plot(voiture_deux_places, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule camionnette standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "camionnette_standard = filter(row -> row.type == \"camionnette_standard\", data)\n",
    "Gadfly.plot(camionnette_standard, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule mini compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_minicompacte = filter(row -> row.type == \"voiture_minicompacte\", data)\n",
    "Gadfly.plot(voiture_minicompacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule VUS standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "VUS_standard = filter(row -> row.type == \"VUS_standard\", data)\n",
    "Gadfly.plot(VUS_standard, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consommation en fonction du type véhicule sous-compacte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_default_plot_size(20cm, 20cm)\n",
    "voiture_sous_compacte = filter(row -> row.type == \"voiture_sous_compacte\", data)\n",
    "Gadfly.plot(voiture_sous_compacte, x=:annee, y=:consommation, color=:type, Geom.point, Geom.smooth(method=:loess), Guide.xlabel(\"Année\"), Guide.ylabel(\"Consommation (L/100km)\"), Guide.colorkey(\"Type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cylindrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nombre de cylindres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transmission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ce qui concerne le type de transmission, nous avons commencé par observer la distribution des catégories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [data]\n",
    "    println(\"Nombre d'observations par type de transmission :\")\n",
    "    println(combine(groupby(df, :transmission), nrow))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir, les types de transmission sont représentées inégalement dans l'ensemble de données. Les transmissions integrale et à traction sont largement représentées, tandis que les transmissions à propulsion et 4x4 sont sous-représentées. Cela nous mène à questionner l'effet de ce manque sur les prédictions. Effectivement, si les types de transmission sont corrélés à la consommation d'essence, le fait que certaines catégories soient sous-représentées pourrait biaiser nos prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme de la consommation moyenne en fonction du type de transmission\n",
    "set_default_plot_size(20cm, 20cm)\n",
    "\n",
    "mean_consommation = combine(groupby(data, :transmission), :consommation => mean => :mean_consommation)\n",
    "# moyenne par type de transmission\n",
    "println(mean_consommation)\n",
    "\n",
    "Gadfly.plot(\n",
    "    mean_consommation,\n",
    "    x=:transmission,\n",
    "    y=:mean_consommation,\n",
    "    Geom.bar,\n",
    "    Guide.xlabel(\"Transmission\"),\n",
    "    Guide.ylabel(\"Consommation moyenne (L/100km)\"),\n",
    "    Guide.title(\"Consommation moyenne en fonction du type de transmission\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme, les prédictions risquent d'être biaisées vers des valeurs associées aux classes majoritaires (\"integrale\" ou \"traction\"), nous pourrions poser l'hypothèse que le modèle risque de sous-estimer non seulement la consommation des véhicules avec une transmission 4x4 et propulsion, mais aussi les prédictions globales en raison de la forte représentation de la transmission traction. (Voir histogramme de la consommation par type de transmission). Pour pallier à ce problème, nous pourrions envisager de pondérer ou de regrouper les classes de transmission pour équilibrer les données. Par exemple, nous pourrions ajouter un poids élevé à la classe \"4x4\" et un poids plus faible à la classe \"Traction\" pour compenser leur représentation respective. Nous pourrions également regrouper les classes \"Intégrale\" et \"Propulsion\" en une seule catégorie pour augmenter leur représentation dans l'ensemble de données, si leur distinction n'apporte pas de valeur ajoutée au modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline model linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode 'boite' column in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.boite = ifelse.(df.boite .== \"automatique\", 1.0, 0.0)\n",
    "end\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with all variables\n",
    "\n",
    "model = lm(@formula(consommation ~ transmission_propulsion + transmission_traction + transmission_integrale + transmission_4x4), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction avec l'ensemble de validation\n",
    "valid_prediction = GLM.predict(model, valid)\n",
    "# Trouver la moyenne de prediction\n",
    "mean_prediction = mean(valid_prediction)\n",
    "# Remplacer les missing par la moyenne\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "# Transformer les predictions en valeur entiere\n",
    "#v = Int.(round.(valid_prediction, digits=0)) #mettre une commentaire sur la difference que ca entraine sur le rmse\n",
    "# Calculer le RMSE\n",
    "rmse_valid = sqrt(mean((valid_prediction - valid.consommation).^2))\n",
    "println(\"RMSE: \", rmse_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE de base: 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Approche A: Ajustement de poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = CSV.read(\"../data/raw/train.csv\", DataFrame; delim=\";\")\n",
    "test =  CSV.read(\"../data/raw/test.csv\", DataFrame; delim=\";\") #ne contient pas la varialbe consommation\n",
    "\n",
    "Random.seed!(1234) #pour la reproductibilit\n",
    "\n",
    "ntrain = round(Int, .8*nrow(full_train)) #80% des données pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #échantillonnage aléatoire pour l'entrainement\n",
    "valid_id = setdiff(1:nrow(full_train), train_id) #échantillon de validation. prend celles qui ne sont pas dans l'échantillon d'entrainement\n",
    "\n",
    "train = full_train[train_id, :]  \n",
    "valid = full_train[valid_id, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [train, valid]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [train, valid, test]\n",
    "    dropmissing!(df)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = combine(groupby(train, :transmission), nrow => :nrow)\n",
    "\n",
    "# Step 2: Calculate total number of observations and number of classes\n",
    "total_samples = sum(counts.nrow)\n",
    "num_classes = nrow(counts)\n",
    "\n",
    "# Step 3: Compute weights for each transmission type\n",
    "counts.Weight = total_samples ./ (num_classes .* counts.nrow)\n",
    "\n",
    "# Step 4: Merge weights back into the original DataFrame\n",
    "train = leftjoin(train, counts[:, [:transmission, :Weight]], on=:transmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [:transmission,:type]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(train)\n",
    "dropmissing!(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuster le modèle avec les poids\n",
    "\n",
    "@time modelA = fit(LinearModel, @formula(consommation ~ transmission_propulsion + transmission_traction + transmission_integrale + transmission_4x4), train, wts = train.Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction avec l'ensemble de validation\n",
    "valid_prediction = GLM.predict(modelA, valid)\n",
    "train_prediction = GLM.predict(modelA, train)\n",
    "# Trouver la moyenne de prediction\n",
    "mean_prediction = mean(valid_prediction)\n",
    "mean_prediction_train = mean(train_prediction)\n",
    "# Remplacer les missing par la moyenne\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "train_prediction = coalesce.(train_prediction, mean_prediction_train)\n",
    "# Transformer les predictions en valeur entiere\n",
    "#v = Int.(round.(valid_prediction, digits=0)) #mettre une commentaire sur la difference que ca entraine sur le rmse\n",
    "# Calculer le RMSE\n",
    "rmse_valid = sqrt(mean((valid_prediction - valid.consommation).^2))\n",
    "rmse_train = sqrt(mean((train_prediction - train.consommation).^2))\n",
    "println(\"RMSE: \", rmse_valid)\n",
    "println(\"RMSE train: \", rmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'approche A, le rmse nous donne 1.081 ce qui signifie que les prédicitons ne sont pas affectées positivement à l'ajout de poids selon la transmissions. Nous avons donc décidé de ne pas utiliser cette approche. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Approche B: Combinaison de catégories similaires \n",
    "\n",
    "Nous devons tester la significativité statistique de la différence entre les catégories de transmission. Pour ce faire, nous avons effectué un test de student pour comparer les moyennes de consommation d'essence entre les différentes catégories de transmission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = CSV.read(\"../data/raw/train.csv\", DataFrame; delim=\";\")\n",
    "test =  CSV.read(\"../data/raw/test.csv\", DataFrame; delim=\";\") #ne contient pas la varialbe consommation\n",
    "\n",
    "Random.seed!(1234) #pour la reproductibilit\n",
    "\n",
    "ntrain = round(Int, .8*nrow(full_train)) #80% des données pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #échantillonnage aléatoire pour l'entrainement\n",
    "valid_id = setdiff(1:nrow(full_train), train_id) #échantillon de validation. prend celles qui ne sont pas dans l'échantillon d'entrainement\n",
    "\n",
    "train = full_train[train_id, :]  \n",
    "valid = full_train[valid_id, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = deepcopy(train)\n",
    "valid_data = deepcopy(valid)\n",
    "\n",
    "# Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [train_data, valid_data]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [train_data, valid_data, test]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [train_data, valid_data, test]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [train_data, valid_data, test]\n",
    "    dropmissing!(df)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer la moyenne\n",
    "\n",
    "function t_test_equal_variances(group1, group2)\n",
    "    # Tailles des échantillons\n",
    "    n1 = length(group1)\n",
    "    n2 = length(group2)\n",
    "\n",
    "    # Moyennes des deux groupes\n",
    "    μ1 = mean(group1)\n",
    "    μ2 = mean(group2)\n",
    "\n",
    "    # Variances des deux groupes\n",
    "    s1² = var(group1)\n",
    "    s2² = var(group2)\n",
    "\n",
    "    # Variance combinée pondérée\n",
    "    sp² = ((n1 - 1) * s1² + (n2 - 1) * s2²) / (n1 + n2 - 2)\n",
    "\n",
    "    # Statistique t\n",
    "    t_stat = (μ1 - μ2) / sqrt(sp² * (1 / n1 + 1 / n2))\n",
    "\n",
    "    # Degrés de liberté\n",
    "    df = n1 + n2 - 2\n",
    "\n",
    "    # p-valeur approximative (bilatérale) avec la loi de Student\n",
    "    # Approximation pour t-distribution : 2 * (1 - cdf(gaussian, abs(t_stat)))\n",
    "    # ou intégrer avec des librairies de tables t pour une solution complète.\n",
    "    return t_stat, df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemple d'utilisation\n",
    "group_integrale = train_data[train_data.transmission .== \"integrale\", :consommation]\n",
    "group_propulsion = train_data[train_data.transmission .== \"propulsion\", :consommation]\n",
    "\n",
    "# Calcul du test t\n",
    "t_stat, df = t_test_equal_variances(group_integrale, group_propulsion)\n",
    "\n",
    "absolut_t_stat = abs(t_stat)\n",
    "\n",
    "pvalue = 2 * (1 - cdf(TDist(df), absolut_t_stat))\n",
    "\n",
    "println(\"Statistique t : $absolut_t_stat\")\n",
    "println(\"p-valeur : $pvalue\")\n",
    "println(\"Degrés de liberté : $df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme le test de student nous donne une p-value > 0.05, nous ne pouvons pas rejeter l'hypothèse nulle selon laquelle les moyennes de consommation d'essence entre les différentes catégories de transmission sont égales. Cela signifie que les différences de consommation d'essence entre les catégories de transmission ne sont pas statistiquement significatives. Par conséquent, nous pouvons regrouper les catégories de transmission similaires pour augmenter leur représentation dans l'ensemble de données sans affecter la qualité des prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the grouped categories\n",
    "# if transmission is 4x4, change for integrale\n",
    "train_data.transmission = ifelse.(train_data.transmission .== \"4x4\", \"integrale\", train_data.transmission)\n",
    "valid_data.transmission = ifelse.(valid_data.transmission .== \"4x4\", \"integrale\", valid_data.transmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode 'boite' column in all datasets\n",
    "for df in [train_data, valid_data, test]\n",
    "    df.boite = ifelse.(df.boite .== \"automatique\", 1.0, 0.0)\n",
    "end\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train_data[!, col])\n",
    "end\n",
    "\n",
    "train_data = one_hot_encode(train_data, categorical_cols, levels_dict)\n",
    "valid_data = one_hot_encode(valid_data, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)\n",
    "\n",
    "println(names(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time modelB = lm(@formula(consommation ~ \n",
    "    transmission_integrale + transmission_traction + transmission_propulsion), train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction avec l'ensemble de validation\n",
    "valid_prediction = GLM.predict(modelB, valid_data)\n",
    "# Trouver la moyenne de prediction\n",
    "mean_prediction = mean(valid_prediction)\n",
    "# Remplacer les missing par la moyenne\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "# Transformer les predictions en valeur entiere\n",
    "#v = Int.(round.(valid_prediction, digits=0)) #mettre une commentaire sur la difference que ca entraine sur le rmse\n",
    "# Calculer le RMSE\n",
    "rmse_valid = sqrt(mean((valid_prediction - valid_data.consommation).^2))\n",
    "println(\"RMSE: \", rmse_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrélation entre les variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [:annee, :nombre_cylindres, :cylindree, :consommation]\n",
    "\n",
    "M = cor(Matrix(data[:, numeric_cols]))\n",
    "\n",
    "# Afficher la matrice de corrélation\n",
    "println(\"Matrice de corrélation :\")\n",
    "println(M)\n",
    "\n",
    "# PLOT\n",
    "(n,m) = size(M)\n",
    "heatmap(M, fc=cgrad([:white,:dodgerblue4]), xticks=(1:m,numeric_cols), xrot=90, yticks=(1:m,numeric_cols), yflip=true)\n",
    "annotate!([(j, i, text(round(M[i,j],digits=3), 8,\"Computer Modern\",:black)) for i in 1:n for j in 1:m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `nombre_cylindres` et `cylindree` est très élevée, ce qui indique une forte relation positive. Cela suggère que le nombre de cylindres est fortement associé à la cylindrée des véhicules.\n",
    "\n",
    "2. La corrélation entre `cylindree` et `consommation` est également élevée, montrant qu'une augmentation de la cylindrée est associée à une augmentation de la consommation (par exemple, les moteurs plus gros consomment plus de carburant).\n",
    "\n",
    "3. Une corrélation similaire existe entre `nombre_cylindres` et `consommation`, ce qui est logique, car le nombre de cylindres et la cylindrée sont liés.\n",
    "\n",
    "4. Les corrélations entre annee et les autres variables sont faibles et négatives, indiquant que les variables comme le nombre de cylindres, la cylindrée et la consommation ont légèrement diminué avec le temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Régression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = CSV.read(\"../data/raw/train.csv\", DataFrame; delim=\";\")\n",
    "test =  CSV.read(\"../data/raw/test.csv\", DataFrame; delim=\";\") #ne contient pas la varialbe consommation\n",
    "\n",
    "Random.seed!(1234) #pour la reproductibilit\n",
    "\n",
    "ntrain = round(Int, .8*nrow(full_train)) #80% des données pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #échantillonnage aléatoire pour l'entrainement\n",
    "valid_id = setdiff(1:nrow(full_train), train_id) #échantillon de validation. prend celles qui ne sont pas dans l'échantillon d'entrainement\n",
    "\n",
    "train = full_train[train_id, :]  \n",
    "valid = full_train[valid_id, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capacite_moteur = :nombre_cylindres * :cylindree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert annee column into age\n",
    "train.age = 2024 .- train.annee\n",
    "valid.age = 2024 .- valid.annee\n",
    "test.age = 2024 .- test.annee\n",
    "\n",
    "train = select!(train, Not(:annee))\n",
    "valid = select!(valid, Not(:annee))\n",
    "test = select!(test, Not(:annee))\n",
    "\n",
    "## drop missing values\n",
    "train = dropmissing(train)\n",
    "valid = dropmissing(valid)\n",
    "test = dropmissing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prétraitement\n",
    "\n",
    "# Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [train, valid]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [train, valid, test]\n",
    "    dropmissing!(df)\n",
    "end\n",
    "\n",
    "# # Encode 'boite' column in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.boite = ifelse.(df.boite .== \"automatique\", 1.0, 0.0)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new column with the grouped categories\n",
    "# # if transmission is 4x4, change for integrale\n",
    "# train.transmission = ifelse.(train.transmission .== \"4x4\", \"integrale\", train.transmission)\n",
    "# train.transmission = ifelse.(train.transmission .== \"4x4\", \"integrale\", train.transmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission, :boite]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "function standardizer(X, means, stds)\n",
    "    X = deepcopy(X)\n",
    "    for j in 1:size(X, 2)\n",
    "        if j in numeric_indices\n",
    "            X[:, j] = (X[:, j] .- means[j]) ./ stds[j]\n",
    "        end\n",
    "    end\n",
    "    return X\n",
    "end\n",
    "\n",
    "# cols_to_normalize = [:age, :capacite_moteur]\n",
    "\n",
    "# means = [mean(train[!, col]) for col in cols_to_normalize]\n",
    "# stds = [std(train[!, col]) for col in cols_to_normalize]\n",
    "\n",
    "# train[!, cols_to_normalize] = standardizer(Matrix(train[!, cols_to_normalize]), means, stds)\n",
    "# valid[!, cols_to_normalize] = standardizer(Matrix(valid[!, cols_to_normalize]), means, stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print every column in the dataframe\n",
    "\n",
    "rename!(train, \"boite_0.0\" => \"boite_0\", \"boite_1.0\" => \"boite_1\")\n",
    "rename!(valid, \"boite_0.0\" => \"boite_0\", \"boite_1.0\" => \"boite_1\")\n",
    "\n",
    "\n",
    "for col in names(train)\n",
    "    println(col)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GLM\n",
    "\n",
    "# Define all formulas for models\n",
    "model_formulas = [\n",
    "    @formula(consommation ~ age),\n",
    "    @formula(consommation ~ age + nombre_cylindres),\n",
    "    @formula(consommation ~ age + cylindree),\n",
    "    @formula(consommation ~ age + nombre_cylindres + cylindree ),\n",
    "    @formula(consommation ~ age + transmission_integrale),\n",
    "    @formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + nombre_cylindres),\n",
    "    @formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree),\n",
    "    @formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree + nombre_cylindres),\n",
    "    @formula(consommation ~ nombre_cylindres + cylindree + age + \n",
    "    type_voiture_moyenne + type_VUS_petit + type_voiture_compacte + type_voiture_deux_places + type_voiture_minicompacte +\n",
    "    type_VUS_standard + type_voiture_sous_compacte + type_break_petit + type_voiture_grande + type_camionnette_standard +\n",
    "    transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + boite_0 + boite_1),\n",
    "    @formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree + type_voiture_moyenne + type_VUS_petit + type_voiture_compacte + type_voiture_deux_places + type_voiture_minicompacte +\n",
    "    type_VUS_standard + type_voiture_sous_compacte + type_break_petit + type_voiture_grande + type_camionnette_standard),\n",
    "    @formula(consommation ~ age + cylindree + type_voiture_moyenne + type_VUS_petit + type_voiture_compacte + type_voiture_deux_places + type_voiture_minicompacte +\n",
    "    type_VUS_standard + type_voiture_sous_compacte + type_break_petit + type_voiture_grande + type_camionnette_standard)\n",
    "]\n",
    "\n",
    "# Fit each model and calculate BIC\n",
    "# Fit models and calculate BIC\n",
    "models = []\n",
    "bics = []\n",
    "rmses = []\n",
    "for formula in model_formulas\n",
    "    model = lm(formula, train)\n",
    "    push!(models, model)\n",
    "    push!(bics, GLM.bic(model))\n",
    "    \n",
    "    # Calculate RMSE on validation set\n",
    "    valid_prediction = GLM.predict(model, valid)\n",
    "    rmse = sqrt(mean((valid_prediction - valid.consommation).^2))\n",
    "    push!(rmses, rmse)\n",
    "end\n",
    "\n",
    "# Find the best model by BIC and RMSE\n",
    "best_bic_index = argmin(bics)\n",
    "best_rmse_index = argmin(rmses)\n",
    "\n",
    "println(\"Best Model by BIC: \", model_formulas[best_bic_index])\n",
    "println(\"BIC: \", bics[best_bic_index])\n",
    "println(\"Best Model by RMSE: \", model_formulas[best_rmse_index])\n",
    "println(\"RMSE: \", rmses[best_rmse_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lm(@formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree), train)\n",
    "#model = lm(@formula(consommation ~ age + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + nombre_cylindres), train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction avec l'ensemble de validation\n",
    "valid_prediction = GLM.predict(model, valid)\n",
    "# Trouver la moyenne de prediction\n",
    "mean_prediction = mean(valid_prediction)\n",
    "# Remplacer les missing par la moyenne\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "# Transformer les predictions en valeur entiere\n",
    "#v = Int.(round.(valid_prediction, digits=0)) #mettre une commentaire sur la difference que ca entraine sur le rmse\n",
    "# Calculer le RMSE\n",
    "rmse_valid = sqrt(mean((valid_prediction - valid.consommation).^2))\n",
    "println(\"RMSE: \", rmse_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nrow(test)\n",
    "\n",
    "id = 1:n\n",
    "\n",
    "ŷ = GLM.predict(model, test)\n",
    "\n",
    "df_pred = DataFrame(id=id, consommation=ŷ)\n",
    "\n",
    "name = \"linear/\" * string(rmse_valid) * \".csv\"\n",
    "CSV.write(\"../submissions/\" * name, df_pred)\n",
    "println(\"Predictions exported successfully to \" * name*\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Régression bayesienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert annee column into age\n",
    "train.age = 2024 .- train.annee\n",
    "valid.age = 2024 .- valid.annee\n",
    "test.age = 2024 .- test.annee\n",
    "\n",
    "train = select!(train, Not(:annee))\n",
    "valid = select!(valid, Not(:annee))\n",
    "test = select!(test, Not(:annee))\n",
    "\n",
    "## drop missing values\n",
    "train = dropmissing(train)\n",
    "valid = dropmissing(valid)\n",
    "test = dropmissing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets that contain 'consommation'\n",
    "datasets_with_consommation = [train, valid]\n",
    "\n",
    "# Datasets without 'consommation'\n",
    "datasets_without_consommation = [test]\n",
    "\n",
    "# Apply replacements to 'cylindree' in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = replace.(df.cylindree, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Apply replacements to 'consommation' only in datasets that have it\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = replace.(df.consommation, \",\" => \".\")\n",
    "end\n",
    "\n",
    "# Convert 'cylindree' to float in all datasets\n",
    "for df in [train, valid, test]\n",
    "    df.cylindree = safe_parse_float.(df.cylindree)\n",
    "end\n",
    "\n",
    "# Convert 'consommation' to float in datasets with 'consommation'\n",
    "for df in datasets_with_consommation\n",
    "    df.consommation = safe_parse_float.(df.consommation)\n",
    "end\n",
    "\n",
    "# Drop missing values in all datasets\n",
    "for df in [train, valid, test]\n",
    "    dropmissing!(df)\n",
    "end\n",
    "\n",
    "# # Encode 'boite' column in all datasets\n",
    "# for df in [train, valid, test]\n",
    "#     df.boite = ifelse.(df.boite .== \"automatique\", 1.0, 0.0)\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission, :boite]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.consommation\n",
    "X_train = select(train, Not(:consommation))\n",
    "y_valid = valid.consommation\n",
    "X_valid = select(valid, Not(:consommation))\n",
    "X_test = deepcopy(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric feature indices\n",
    "feature_names = names(train)\n",
    "numeric_features = [ :cylindree, :nombre_cylindres, :age]\n",
    "numeric_indices = findall(x -> x in numeric_features, feature_names)\n",
    "\n",
    "means = mean(Matrix(X_train[:, numeric_features]), dims=1)\n",
    "stds = std(Matrix(X_train[:, numeric_features]), dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function standardizer(X, means, stds)\n",
    "    X = deepcopy(X)\n",
    "    for j in 1:size(X, 2)\n",
    "        if j in numeric_indices\n",
    "            X[:, j] = (X[:, j] .- means[j]) ./ stds[j]\n",
    "        end\n",
    "    end\n",
    "    return X\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = standardizer(Matrix(X_train), means, stds)\n",
    "X_valid = standardizer(Matrix(X_valid), means, stds)\n",
    "X_test = standardizer(Matrix(X_test), means, stds)\n",
    "\n",
    "y_train = Vector(y_train)\n",
    "y_valid = Vector(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression with cross-validation\n",
    "XtX = X_train' * X_train\n",
    "Xty = X_train' * y_train\n",
    "n_features = size(X_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values = 10 .^ range(-5, stop=5, length=1000)\n",
    "best_rmse = Inf\n",
    "best_lambda = 0.0\n",
    "best_beta = nothing\n",
    "\n",
    "\n",
    "for λ in lambda_values\n",
    "    beta = (XtX + λ * I) \\ Xty\n",
    "    y_pred_valid = X_valid * beta\n",
    "    rmse = sqrt(mean((y_pred_valid - y_valid).^2))\n",
    "    if rmse < best_rmse\n",
    "        best_rmse = rmse\n",
    "        best_lambda = λ\n",
    "        best_beta = beta\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Best Lambda: \", best_lambda)\n",
    "println(\"Best RMSE: \", best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation on validation set\n",
    "y_valid_pred = X_valid * best_beta\n",
    "rmse_valid = sqrt(mean((y_valid_pred - y_valid).^2))\n",
    "println(\"Validation RMSE: \", rmse_valid)\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred = X_test * best_beta\n",
    "\n",
    "# Prepare submission DataFrame\n",
    "n_test = size(y_test_pred, 1)\n",
    "id = 1:n_test\n",
    "df_pred = DataFrame(id=id, consommation=y_test_pred)\n",
    "\n",
    "name = \"ridge\" * string(rmse_valid) * \".csv\"\n",
    "CSV.write(\"../submissions/bayes/\" * name, df_pred)\n",
    "println(\"Predictions exported successfully to \" * name*\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation par k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k_folds = vcat(train, valid)\n",
    "y = data_k_folds.consommation\n",
    "X = select(data_k_folds, Not(:consommation))\n",
    "\n",
    "n = nrow(data_k_folds)\n",
    "k = 5  \n",
    "fold_size = n ÷ k\n",
    "\n",
    "indices = randperm(n)\n",
    "\n",
    "rms_scores = []\n",
    "\n",
    "for i in 0:(k-1)\n",
    "    test_indices = indices[(i*fold_size + 1):min((i+1)*fold_size, n)]\n",
    "    train_indices = setdiff(indices, test_indices)\n",
    "    \n",
    "    train_data = data_k_folds[train_indices, :]\n",
    "    test_data = data_k_folds[test_indices, :]\n",
    "    \n",
    "    model = lm(@formula(consommation ~ age  + transmission_integrale + transmission_propulsion + transmission_traction + transmission_4x4 + cylindree), data_k_folds)\n",
    " \n",
    "    \n",
    "    valid_prediction = GLM.predict(model, test_data)\n",
    "    \n",
    "    mean_prediction = mean(skipmissing(valid_prediction))\n",
    "    valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "    \n",
    "    if any(ismissing, valid_prediction)\n",
    "        error(\"Skip les valeur missing\")\n",
    "    end\n",
    "    \n",
    "    v = max.(valid_prediction, 0) \n",
    "    \n",
    "    score = sqrt(mean((v - test_data.consommation).^2))\n",
    "    push!(rms_scores, score)\n",
    "end\n",
    "\n",
    "moyenne_rmse = mean(rms_scores)\n",
    "println(\"Moyenne RMSE : $moyenne_rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression par l'approche des composantes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234) # For reproducibility\n",
    "\n",
    "# Split the data\n",
    "ntrain = round(Int, 0.8 * nrow(full_train))\n",
    "train_id = sample(1:nrow(full_train), ntrain; replace=false, ordered=true)\n",
    "valid_id = setdiff(1:nrow(full_train), train_id)\n",
    "\n",
    "train = full_train[train_id, :]\n",
    "valid = full_train[valid_id, :]\n",
    "\n",
    "# Data cleaning\n",
    "for col in [:cylindree, :consommation]\n",
    "    train[!, col] = replace.(train[!, col], \",\" => \".\")\n",
    "    valid[!, col] = replace.(valid[!, col], \",\" => \".\")\n",
    "    train[!, col] = safe_parse_float.(train[!, col])\n",
    "    valid[!, col] = safe_parse_float.(valid[!, col])\n",
    "end\n",
    "\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "# train = select(train, Not([:type, :transmission, :boite]))\n",
    "# valid = select(valid, Not([:type, :transmission, :boite]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TODO CONCLUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = setdiff(names(train)[eltype.(eachcol(train)) .<: Real], [:consommation])\n",
    "X_train = Matrix(select(train, numeric_cols))\n",
    "\n",
    "y_train = Vector(train.consommation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = mean(X_train; dims=1)\n",
    "X_stddev = std(X_train; dims=1, corrected=false)\n",
    "X_train_std = (X_train .- X_mean) ./ X_stddev\n",
    "X_valid = Matrix(select(valid, numeric_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = CSV.read(\"../data/raw/train.csv\", DataFrame; delim=\";\")\n",
    "test =  CSV.read(\"../data/raw/test.csv\", DataFrame; delim=\";\") #ne contient pas la varialbe consommation\n",
    "\n",
    "Random.seed!(1234) #pour la reproductibilit\n",
    "\n",
    "ntrain = round(Int, .8*nrow(full_train)) #80% des données pour l'entrainement: 80% * nb de lignes\n",
    "\n",
    "train_id = sample(1:nrow(full_train), ntrain, replace=false, ordered=true) #échantillonnage aléatoire pour l'entrainement\n",
    "valid_id = setdiff(1:nrow(full_train), train_id) #échantillon de validation. prend celles qui ne sont pas dans l'échantillon d'entrainement\n",
    "\n",
    "train = full_train[train_id, :]  \n",
    "valid = full_train[valid_id, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MultivariateStats\n",
    "using GLM\n",
    "\n",
    "# Data Cleaning and Preparation\n",
    "Random.seed!(1234) # For reproducibility\n",
    "\n",
    "# Split the data\n",
    "ntrain = round(Int, 0.8 * nrow(full_train))\n",
    "train_id = sample(1:nrow(full_train), ntrain; replace=false, ordered=true)\n",
    "valid_id = setdiff(1:nrow(full_train), train_id)\n",
    "\n",
    "train = full_train[train_id, :]\n",
    "valid = full_train[valid_id, :]\n",
    "\n",
    "# Data cleaning\n",
    "for col in [:cylindree, :consommation]\n",
    "    train[!, col] = replace.(train[!, col], \",\" => \".\")\n",
    "    valid[!, col] = replace.(valid[!, col], \",\" => \".\")\n",
    "    train[!, col] = safe_parse_float.(train[!, col])\n",
    "    valid[!, col] = safe_parse_float.(valid[!, col])\n",
    "end\n",
    "\n",
    "for col in [:cylindree]\n",
    "    test[!, col] = replace.(test[!, col], \",\" => \".\")\n",
    "    test[!, col] = safe_parse_float.(test[!, col])\n",
    "end\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "test = one_hot_encode(test, categorical_cols, levels_dict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric columns\n",
    "# numeric_cols = select(train, Not(:consommation))\n",
    "\n",
    "numeric_cols_test = setdiff(names(test)[eltype.(eachcol(test)) .<: Real])\n",
    "\n",
    "# Extract numeric columns\n",
    "X_train = Matrix(select(train, Not(:consommation, )))\n",
    "y_train = Vector(train.consommation)\n",
    "X_valid = Matrix(select(train, Not(:consommation)))\n",
    "y_valid = Vector(valid.consommation)\n",
    "X_test = Matrix(select(train, Not(:consommation)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "X_mean = mean(X_train; dims=1)\n",
    "X_stddev = std(X_train; dims=1, corrected=false)\n",
    "X_train_std = (X_train .- X_mean) ./ X_stddev\n",
    "X_valid_std = (X_valid .- X_mean) ./ X_stddev\n",
    "#X_test_std = (X_test .- X_mean) ./ X_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca_model = fit(PCA, X_train_std'; maxoutdim=5)\n",
    "Z_train = MultivariateStats.transform(pca_model, X_train_std')'\n",
    "Z_valid = MultivariateStats.transform(pca_model, X_valid_std')'\n",
    "#Z_test = MultivariateStats.transform(pca_model, X_test_std')'\n",
    "\n",
    "# Add principal components to DataFrames (train and valid)\n",
    "for i in 1:size(Z_train, 2)\n",
    "    train[:, \"PC$(i)\"] = Z_train[:, i]\n",
    "    valid[:, \"PC$(i)\"] = Z_valid[:, i]\n",
    "    #test[:, \"PC$(i)\"] = Z_test[:, i]\n",
    "end\n",
    "\n",
    "# Fit a regression model using PCA\n",
    "model_with_pca = lm(@formula(consommation ~ PC1 + PC2 + PC3 + PC4 + PC5), train)\n",
    "\n",
    "# Predict on validation data\n",
    "valid_prediction_with_pca = predict(model_with_pca, valid)\n",
    "#test_prediction_with_pca = predict(model_with_pca, test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_with_pca = sqrt(mean((valid_prediction_with_pca - valid.consommation).^2))\n",
    "println(\"RMSE with PCA: \", rmse_with_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission DataFrame\n",
    "n_test = size(test_prediction_with_pca, 1)\n",
    "id = 1:n_test\n",
    "df_pred = DataFrame(id=id, consommation=test_prediction_with_pca)\n",
    "\n",
    "name = \"pca\" * string(rmse_with_pca) * \".csv\"\n",
    "CSV.write(\"../submissions/\" * name, df_pred)\n",
    "println(\"Predictions exported successfully to \" * name*\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MultivariateStats\n",
    "using GLM\n",
    "using Random\n",
    "using DataFrames\n",
    "\n",
    "\n",
    "# Data Cleaning and Preparation\n",
    "Random.seed!(1234) # For reproducibility\n",
    "\n",
    "# Split the data into training, validation, and testing datasets\n",
    "ntotal = nrow(full_train)\n",
    "ntrain = round(Int, 0.8 * ntotal)\n",
    "nvalid = round(Int, 0.2 * ntotal)\n",
    "\n",
    "train_id = sample(1:ntotal, ntrain; replace=false, ordered=true)\n",
    "remaining_id = setdiff(1:ntotal, train_id)\n",
    "valid_id = sample(remaining_id, nvalid; replace=false, ordered=true)\n",
    "\n",
    "train = full_train[train_id, :]\n",
    "valid = full_train[valid_id, :]\n",
    "\n",
    "# Data cleaning\n",
    "for col in [:cylindree, :consommation, :nombre_cylindres]\n",
    "    for df in [train, valid]\n",
    "        df[!, col] = replace.(df[!, col], \",\" => \".\")\n",
    "        df[!, col] = safe_parse_float.(df[!, col])\n",
    "    end\n",
    "end\n",
    "\n",
    "for col in [:cylindree]\n",
    "    test[!, col] = replace.(test[!, col], \",\" => \".\")\n",
    "    test[!, col] = safe_parse_float.(test[!, col])\n",
    "end\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "for df in [train, valid, test]\n",
    "    one_hot_encode!(df, categorical_cols, levels_dict)\n",
    "end\n",
    "\n",
    "# Define numeric columns\n",
    "numeric_cols = setdiff(names(train)[eltype.(eachcol(train)) .<: Real], [:consommation])\n",
    "\n",
    "# Extract numeric columns\n",
    "X_train = Matrix(select(train, numeric_cols))\n",
    "y_train = Vector(train.consommation)\n",
    "X_valid = Matrix(select(valid, numeric_cols))\n",
    "y_valid = Vector(valid.consommation)\n",
    "#X_test = Matrix(select(test, numeric_cols))\n",
    "\n",
    "# Standardize the data\n",
    "X_mean = mean(X_train; dims=1)\n",
    "X_stddev = std(X_train; dims=1, corrected=false)\n",
    "X_train_std = (X_train .- X_mean) ./ X_stddev\n",
    "X_valid_std = (X_valid .- X_mean) ./ X_stddev\n",
    "#X_test_std = (X_test .- X_mean) ./ X_stddev\n",
    "\n",
    "# Perform PCA\n",
    "pca_model = fit(PCA, X_train_std'; maxoutdim=5)\n",
    "Z_train = MultivariateStats.transform(pca_model, X_train_std')'\n",
    "Z_valid = MultivariateStats.transform(pca_model, X_valid_std')'\n",
    "#Z_test = MultivariateStats.transform(pca_model, X_test_std')'\n",
    "\n",
    "# Add principal components to DataFrames (train, valid, and test)\n",
    "for i in 1:size(Z_train, 2)\n",
    "    train[:, \"PC$(i)\"] = Z_train[:, i]\n",
    "    valid[:, \"PC$(i)\"] = Z_valid[:, i]\n",
    "    #test[:, \"PC$(i)\"] = Z_test[:, i]\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a regression model using PCA\n",
    "model_with_pca = lm(@formula(consommation ~ PC1 + PC2 + PC3 + PC4 + PC5), train)\n",
    "\n",
    "# Predict on validation and testing data\n",
    "valid_prediction_with_pca = predict(model_with_pca, valid)\n",
    "test_prediction_with_pca = predict(model_with_pca, test)\n",
    "\n",
    "# Calculate RMSE for validation and testing datasets\n",
    "rmse_with_pca_valid = sqrt(mean((valid_prediction_with_pca - y_valid).^2))\n",
    "# rmse_with_pca_test = sqrt(mean((test_prediction_with_pca - y_test).^2))\n",
    "\n",
    "println(\"RMSE on Validation Set: \", rmse_with_pca_valid)\n",
    "# println(\"RMSE on Testing Set: \", rmse_with_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décomposer en composantes principales\n",
    "pca_model = fit(PCA, X_train_std'; maxoutdim=8)\n",
    "\n",
    "# T = Z * V pour la matrice des composantes principales.\n",
    "Z_train = MultivariateStats.transform(pca_model, X_train_std')\n",
    "Z_train = Z_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model de regression sur les composantes principales\n",
    "model = lm(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centrer - réduire les données de validation\n",
    "\n",
    "numeric_cols = setdiff(names(valid)[eltype.(eachcol(train)) .<: Real], [:consommation])\n",
    "X_valid = Matrix(select(valid, numeric_cols))\n",
    "\n",
    "X_valid_std = (X_valid .- X_mean) ./ X_stddev\n",
    "\n",
    "Z_valid = MultivariateStats.transform(pca_model, X_valid_std')\n",
    "Z_valid = Z_valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prediction = predict(model, Z_valid)\n",
    "\n",
    "mean_prediction = mean(skipmissing(valid_prediction))\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "\n",
    "mean_actual = mean(skipmissing(valid.consommation))\n",
    "actual_values = coalesce.(valid.consommation, mean_actual)\n",
    "\n",
    "rmse = sqrt(mean((valid_prediction - actual_values).^2))\n",
    "println(\"RMSE: \", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approche de pupuce \n",
    "# données ne sont pas standartisées ni one hot encoded\n",
    "y = train.consommation\n",
    "X = train[:, Not([:consommation])]\n",
    "\n",
    "X = Matrix(X)\n",
    "y = Vector(y)\n",
    "\n",
    "pca_model = fit(PCA, X, maxoutdim=8)\n",
    "\n",
    "# La PCA est appliquée aux données non standardisées\n",
    "# Cela revient à entraîner le modèle sur une version approximative \n",
    "#des données d'origine, ce qui peut réintroduire la multicolinéarité et annuler certains avantages de la PCA.\n",
    "Yte = predict(pca_model, X)\n",
    "Xr = reconstruct(pca_model, Yte)\n",
    "\n",
    "model = lm(Xr, y) \n",
    "consommation = valid.consommation\n",
    "select!(valid, Not(:consommation));\n",
    "X_valid = Matrix{Float64}(valid);\n",
    "valid_prediction = predict(model, X_valid)\n",
    "\n",
    "mean_prediction = mean(skipmissing(valid_prediction))\n",
    "\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "\n",
    "mean_actual = mean(skipmissing(consommation))\n",
    "actual_values = coalesce.(consommation, mean_actual)\n",
    "\n",
    "rmse = sqrt(mean((valid_prediction - actual_values).^2))\n",
    "println(\"RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MultivariateStats\n",
    "using GLM\n",
    "\n",
    "Random.seed!(1234) # For reproducibility\n",
    "\n",
    "# Split the data\n",
    "ntrain = round(Int, 0.8 * nrow(full_train))\n",
    "train_id = sample(1:nrow(full_train), ntrain; replace=false, ordered=true)\n",
    "valid_id = setdiff(1:nrow(full_train), train_id)\n",
    "\n",
    "train = full_train[train_id, :]\n",
    "valid = full_train[valid_id, :]\n",
    "\n",
    "# Data cleaning\n",
    "for col in [:cylindree, :consommation]\n",
    "    train[!, col] = replace.(train[!, col], \",\" => \".\")\n",
    "    valid[!, col] = replace.(valid[!, col], \",\" => \".\")\n",
    "    train[!, col] = safe_parse_float.(train[!, col])\n",
    "    valid[!, col] = safe_parse_float.(valid[!, col])\n",
    "end\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = [:type, :transmission]\n",
    "\n",
    "# Collect unique levels from the training set\n",
    "levels_dict = Dict()\n",
    "for col in categorical_cols\n",
    "    levels_dict[col] = unique(train[!, col])\n",
    "end\n",
    "\n",
    "# Apply one-hot encoding\n",
    "train = one_hot_encode(train, categorical_cols, levels_dict)\n",
    "valid = one_hot_encode(valid, categorical_cols, levels_dict)\n",
    "\n",
    "# Drop unnecessary columns (already handled by one_hot_encode if columns are removed)\n",
    "# Select only numeric columns for X\n",
    "numeric_cols = setdiff(names(train)[eltype.(eachcol(train)) .<: Real], [:consommation])\n",
    "X_train = Matrix(select(train, numeric_cols))\n",
    "y_train = Vector(train.consommation)\n",
    "\n",
    "# Fit PCA on non-standardized data\n",
    "pca_model = fit(PCA, X_train; maxoutdim=8)\n",
    "\n",
    "# Transform data\n",
    "Yte = predict(pca_model, X_train)\n",
    "Xr = reconstruct(pca_model, Yte)\n",
    "\n",
    "# Fit the regression model\n",
    "model = lm(Xr, y_train)\n",
    "\n",
    "# Prepare validation data\n",
    "X_valid = Matrix(select(valid, numeric_cols))\n",
    "y_valid = Vector(valid.consommation)\n",
    "\n",
    "# Predict on validation data\n",
    "valid_prediction = predict(model, X_valid)\n",
    "\n",
    "# Handle missing values in predictions\n",
    "mean_prediction = mean(skipmissing(valid_prediction))\n",
    "valid_prediction = coalesce.(valid_prediction, mean_prediction)\n",
    "\n",
    "# Handle missing values in actual values\n",
    "mean_actual = mean(skipmissing(y_valid))\n",
    "actual_values = coalesce.(y_valid, mean_actual)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = sqrt(mean((valid_prediction - actual_values).^2))\n",
    "println(\"RMSE: \", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x̄ = vec(mean(X, dims=1))\n",
    "\n",
    "Z = X .- x̄'\n",
    "\n",
    "F = svd(Z)\n",
    "V = F.V\n",
    "U = F.U\n",
    "γ = F.S;\n",
    "\n",
    "cumvar = cumsum(γ.^2)\n",
    "\n",
    "ratio = cumvar / cumvar[end]\n",
    "\n",
    "df = DataFrame(k = Int64[], Variance = Float64[])\n",
    "\n",
    "for k in 1:length(ratio)\n",
    "    push!(df, [k, ratio[k]])\n",
    "end\n",
    "\n",
    "Gadfly.plot(df, x=:k, y=:Variance, Geom.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Impact des échelles dans les données\n",
    "Dans les données non standardisées, les variables explicatives ayant des valeurs plus grandes peuvent dominer la variance totale, ce qui oriente la PCA vers ces variables. Si ces variables sont fortement corrélées avec la variable cible (\n",
    "𝑌\n",
    "Y), alors le modèle peut accidentellement mieux capturer cette relation.\n",
    "En revanche, la standardisation neutralise les différences d'échelle, ce qui peut diluer l'effet des variables dominantes, même si elles ont une forte corrélation avec \n",
    "𝑌\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Dans votre jeu de données, il est possible qu'une ou plusieurs variables avec des échelles plus grandes soient prédictives de \n",
    "𝑌\n",
    "Y, et la méthode non standardisée en profite directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Sur-ajustement (Overfitting)\n",
    "La méthode non standardisée applique la PCA sur les données d'origine, mais utilise ensuite les données reconstruites (\n",
    "𝑋\n",
    "𝑟\n",
    "X \n",
    "r\n",
    "​\n",
    " ) pour l'entraînement. Cela peut réintroduire une grande partie des informations originales, y compris le bruit ou les corrélations spurielles, ce qui peut conduire à un modèle sur-ajusté.\n",
    "Si l'ensemble de validation est similaire à l'ensemble d'entraînement (par exemple, s'il provient de la même distribution ou a des caractéristiques similaires), un sur-ajustement peut donner des RMSE artificiellement bas.\n",
    "Explication potentielle :\n",
    "Votre validation pourrait être moins rigoureuse, et la méthode non standardisée exploite des relations non généralisables dans les données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Le fait que la méthode non standardisée donne un meilleur RMSE peut s'expliquer par plusieurs raisons, mais cela ne signifie pas nécessairement qu'elle est meilleure ou qu'elle respecte les principes statistiques sous-jacents. Explorons pourquoi cela pourrait se produire :\n",
    "\n",
    "1. Impact des échelles dans les données\n",
    "Dans les données non standardisées, les variables explicatives ayant des valeurs plus grandes peuvent dominer la variance totale, ce qui oriente la PCA vers ces variables. Si ces variables sont fortement corrélées avec la variable cible (\n",
    "𝑌\n",
    "Y), alors le modèle peut accidentellement mieux capturer cette relation.\n",
    "En revanche, la standardisation neutralise les différences d'échelle, ce qui peut diluer l'effet des variables dominantes, même si elles ont une forte corrélation avec \n",
    "𝑌\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Dans votre jeu de données, il est possible qu'une ou plusieurs variables avec des échelles plus grandes soient prédictives de \n",
    "𝑌\n",
    "Y, et la méthode non standardisée en profite directement.\n",
    "\n",
    "2. Sur-ajustement (Overfitting)\n",
    "La méthode non standardisée applique la PCA sur les données d'origine, mais utilise ensuite les données reconstruites (\n",
    "𝑋\n",
    "𝑟\n",
    "X \n",
    "r\n",
    "​\n",
    " ) pour l'entraînement. Cela peut réintroduire une grande partie des informations originales, y compris le bruit ou les corrélations spurielles, ce qui peut conduire à un modèle sur-ajusté.\n",
    "Si l'ensemble de validation est similaire à l'ensemble d'entraînement (par exemple, s'il provient de la même distribution ou a des caractéristiques similaires), un sur-ajustement peut donner des RMSE artificiellement bas.\n",
    "Explication potentielle :\n",
    "Votre validation pourrait être moins rigoureuse, et la méthode non standardisée exploite des relations non généralisables dans les données.\n",
    "\n",
    "3. Corrélation forte entre les variables\n",
    "Si les variables explicatives ont une forte corrélation intrinsèque, l'analyse en composantes principales standardisée peut répartir cette information sur plusieurs composantes. Cela réduit la capacité du modèle à se concentrer sur des variables fortement corrélées avec \n",
    "𝑌\n",
    "Y.\n",
    "La méthode non standardisée, en revanche, conserve ces corrélations et peut donc mieux modéliser la relation entre \n",
    "𝑋\n",
    "X et \n",
    "𝑌\n",
    "Y.\n",
    "Explication potentielle :\n",
    "Les corrélations fortes dans vos données favorisent la méthode non standardisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Effet des données reconstruites\n",
    "Dans la méthode non standardisée, vous utilisez des données reconstruites (\n",
    "𝑋\n",
    "𝑟\n",
    "X \n",
    "r\n",
    "​\n",
    " ), qui incluent une grande partie de l'information originale. Cela signifie que la régression est moins influencée par la réduction de dimension et plus proche de la régression sur les données d'origine.\n",
    "En revanche, dans la méthode standardisée, seules les composantes principales sont utilisées, ce qui peut sacrifier une partie de l'information pour réduire la multicolinéarité et améliorer la généralisation.\n",
    "Explication potentielle :\n",
    "L'utilisation des données reconstruites dans la méthode non standardisée maintient plus d'information, ce qui peut donner un RMSE plus faible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Problème avec la PCA standardisée\n",
    "Si la standardisation n'est pas appropriée (par exemple, si certaines variables explicatives sont presque constantes ou si elles sont déjà sur des échelles comparables), alors l'analyse en composantes principales standardisée peut ne pas capturer efficacement les directions principales de la variance.\n",
    "Cela peut entraîner une perte d'information utile, ce qui affecte les performances du modèle.\n",
    "Explication potentielle :\n",
    "Les variables de votre jeu de données n'ont peut-être pas besoin d'être standardisées ou la standardisation introduit un biais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
